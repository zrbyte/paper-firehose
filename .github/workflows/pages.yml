name: Build and Deploy

on:
  workflow_dispatch:  # Manual trigger
  schedule:
    - cron: '0 4 * * *'  # 04:00 UTC daily

permissions:
  contents: write # Required for uploading DB snapshots to data branch
  pages: write
  id-token: write

# Prevent concurrent runs from corrupting the SQLite database
concurrency:
  group: pages-db
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0    # ensure all branches/refs available

      # Cache the sentence transformer models
      - name: Cache sentence transformer models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-models-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            huggingface-models-

      # Load databases from data branch
      - name: Load databases from data branch
        run: |
          set -euo pipefail
          mkdir -p assets
          
          echo "=== Loading databases from data branch ==="
          echo "Current branch: $(git branch --show-current)"
          
          # Fetch the data branch
          git fetch origin data
          
          # Load all_feed_entries.db from data branch
          if git show origin/data:assets/all_feed_entries.latest.db > assets/all_feed_entries.db 2>/dev/null; then
            echo "âœ… Loaded all_feed_entries.db from data branch"
            ls -lh assets/all_feed_entries.db
          else
            echo "â„¹ï¸  No all_feed_entries.db found in data branch, will initialize fresh"
          fi
          
          # Load matched_entries_history.db from data branch
          if git show origin/data:assets/matched_entries_history.latest.db > assets/matched_entries_history.db 2>/dev/null; then
            echo "âœ… Loaded matched_entries_history.db from data branch"
            ls -lh assets/matched_entries_history.db
          else
            echo "â„¹ï¸  No matched_entries_history.db found in data branch, will initialize fresh"
          fi
          
          echo "=== Database loading complete ==="

      - uses: conda-incubator/setup-miniconda@v3
        with:
          python-version: '3.11'
          use-only-tar-bz2: true

      - run: conda create -n paper-firehose-p311 python=3.11 -y
      - run: conda install -c conda-forge pip -y
      - run: conda run -n paper-firehose-p311 pip install -r requirements.txt

      # Activate conda environment for the pipeline
      - name: Activate conda environment
        run: |
          echo "source /usr/share/miniconda/etc/profile.d/conda.sh" >> $GITHUB_ENV
          echo "conda activate paper-firehose-p311" >> $GITHUB_ENV

      # Run the paper firehose pipeline
      - name: Run paper firehose pipeline
        run: |
          set -euo pipefail
          
          echo "=== Starting Paper Firehose Pipeline ==="
          echo "Current branch: $(git branch --show-current)"
          
          # Ensure we're on main branch (should be by default from checkout)
          if [ "$(git branch --show-current)" != "main" ]; then
            echo "âš ï¸  Not on main branch, switching to main"
            git checkout main
          fi
          
          # Check database state before starting
          echo "Pre-pipeline database check:"
          ls -la assets/*.db 2>/dev/null || echo "No database files found"
          
          # Run the complete pipeline (no timeouts needed - commands should complete normally)
          echo "Step 1: Running filter command..."
          python cli/main.py --verbose filter
          
          echo "Step 2: Running rank command..."
          python cli/main.py --verbose rank
          
          echo "Step 3: Running abstracts command..."
          python cli/main.py --verbose abstracts --mailto "github-actions@paper-firehose.com"
          
          echo "Step 4: Running summarize command..."
          python cli/main.py --verbose summarize
          
          echo "âœ… Pipeline completed successfully"
          
          # Show final database state
          echo "Post-pipeline database check:"
          ls -la assets/*.db 2>/dev/null || echo "No database files found"
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      # Commit updated databases to data branch
      - name: Commit updated databases to data branch
        env:
          KEEP_SNAPSHOTS: "10"
          SNAP_DIR: "assets/db_history"
          DB_HISTORY: "assets/matched_entries_history.db"
          DB_SEEN: "assets/all_feed_entries.db"
          DB_CURRENT: "assets/papers.db"
        run: |
          set -euo pipefail

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git fetch origin --prune

          # Ensure remote data branch exists; create from current HEAD if missing
          if ! git ls-remote --exit-code --heads origin data >/dev/null 2>&1; then
            git branch -f data HEAD
            git push -u origin data || true
          fi

          # Worktree on data branch (create/reset local branch to origin/data)
          WT=/tmp/data-wt
          mkdir -p "$WT"
          git worktree add -B data "$WT" origin/data
          cleanup() { git worktree remove -f "$WT" || true; }
          trap cleanup EXIT

          pushd "$WT" >/dev/null
            mkdir -p "$SNAP_DIR" assets
            TS="$(date -u +%Y%m%dT%H%M%SZ)"

            snapshot_one() {
              local src="$1"; local stem="$2"
              if [ -f "$GITHUB_WORKSPACE/$src" ]; then
                local snap="$SNAP_DIR/${stem}_${TS}.db"
                cp "$GITHUB_WORKSPACE/$src" "$snap"
                cp "$GITHUB_WORKSPACE/$src" "assets/${stem}.latest.db"
                git add -f "$snap" "assets/${stem}.latest.db"
                echo "âœ… Snapshot created: $snap and assets/${stem}.latest.db"
              else
                echo "Skip: $src not found"
              fi
            }

            # All three DBs
            snapshot_one "$DB_HISTORY" "matched_entries_history"
            snapshot_one "$DB_SEEN" "all_feed_entries"
            snapshot_one "$DB_CURRENT" "papers"

            # Retention per DB
            retain() {
              local stem="$1"
              mapfile -t FILES < <(ls -1t "$SNAP_DIR"/${stem}_*.db 2>/dev/null || true)
              if [ "${#FILES[@]}" -gt "${KEEP_SNAPSHOTS}" ]; then
                for f in "${FILES[@]:${KEEP_SNAPSHOTS}}"; do rm -f "$f"; done
                echo "ðŸ—‘ï¸  Retained ${KEEP_SNAPSHOTS} most recent snapshots for $stem"
              fi
            }
            retain "matched_entries_history"
            retain "all_feed_entries"
            retain "papers"

            # Stage deletions from retention
            git add -A "$SNAP_DIR" || true

            if ! git diff --cached --quiet; then
              git commit -m "Update database snapshots @ ${TS} [skip ci] - Updated all database snapshots with latest data - Retained ${KEEP_SNAPSHOTS} most recent snapshots per database - Generated timestamped snapshots in assets/db_history/ - Updated latest database references in assets/"
              git push origin data
              echo "âœ… Database snapshots committed and pushed to data branch"
            else
              echo "â„¹ï¸  No changes to commit."
            fi
          popd >/dev/null

      # Prepare files for GitHub Pages deployment
      - name: Prepare site
        run: |
          mkdir site
          cp -r archive site/ || true
          cp *.html site/ || true

      - uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4