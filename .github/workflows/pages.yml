name: Build and Deploy

on:
  workflow_dispatch:  # Manual trigger
  schedule:
    - cron: '0 4 * * *'  # 04:00 UTC daily

permissions:
  contents: write # Required for uploading DB snapshots to data branch
  pages: write
  id-token: write

# Prevent concurrent runs from corrupting the SQLite database
concurrency:
  group: pages-db
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0    # ensure all branches/refs available

      # Cache the sentence transformer models
      - name: Cache sentence transformer models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-models-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            huggingface-models-

      # Cache the all_feed_entries.db to persist article tracking between runs
      - name: Restore database cache
        uses: actions/cache@v3
        with:
          path: assets/all_feed_entries.db
          key: all-feed-entries-db-${{ github.ref_name }}
          restore-keys: |
            all-feed-entries-db-

      # Cache the matched_entries_history.db to persist historical matches between runs
      # TEMPORARILY DISABLED - will re-enable after bootstrap run
      # - name: Restore history database cache
      #   uses: actions/cache@v3
      #   with:
      #     path: assets/matched_entries_history.db
      #     key: matched-entries-history-db-${{ github.ref_name }}
      #     restore-keys: |
      #       matched-entries-history-db-

      # Debug database state (BOOTSTRAP MODE - using main branch database)
      - name: Debug database state
        run: |
          set -euo pipefail
          mkdir -p assets
          
          echo "=== BOOTSTRAP MODE: Using Main Branch Database ==="
          echo "Current working directory: $(pwd)"
          echo "Assets directory contents:"
          ls -la assets/ || echo "Assets directory does not exist"
          
          if [ -f "assets/matched_entries_history.db" ]; then
            echo "✅ Found matched_entries_history.db from main branch"
            echo "File size: $(ls -lh assets/matched_entries_history.db | awk '{print $5}')"
            echo "File date: $(ls -l assets/matched_entries_history.db | awk '{print $6, $7, $8}')"
            echo "Database source: MAIN BRANCH (cache disabled for bootstrap)"
          else
            echo "❌ No matched_entries_history.db found in main branch"
          fi
          
          echo "=== End Bootstrap Debug ==="

      # Ensure assets dir exists and initialize DBs if cache did not restore them
      - name: Ensure assets dir & init DBs if missing
        run: |
          set -euo pipefail
          mkdir -p assets
          
          # If all_feed_entries.db not present from cache, create an empty one with minimal schema
          if [ ! -f "assets/all_feed_entries.db" ]; then
            echo "Cache miss: initializing assets/all_feed_entries.db"
            python - <<'PY'
          import sqlite3
          import os
          p = 'assets/all_feed_entries.db'
          os.makedirs(os.path.dirname(p), exist_ok=True)
          con = sqlite3.connect(p)
          con.execute('''
          CREATE TABLE IF NOT EXISTS feed_entries (
            entry_id TEXT PRIMARY KEY,
            feed_name TEXT NOT NULL,
            title TEXT NOT NULL,
            link TEXT NOT NULL,
            summary TEXT,
            authors TEXT,
            published_date TEXT,
            first_seen TEXT,
            last_seen TEXT,
            raw_data TEXT
          )''')
          con.commit()
          con.close()
          print('Initialized new all_feed_entries.db')
          PY
          fi
          
          # If matched_entries_history.db not present from cache, create an empty one with minimal schema
          if [ ! -f "assets/matched_entries_history.db" ]; then
            echo "Cache miss: initializing assets/matched_entries_history.db"
            # Use a simpler approach - let the CLI system handle database initialization
            echo "Will let CLI system initialize database if needed"
          fi

      - uses: conda-incubator/setup-miniconda@v3
        with:
          python-version: '3.11'
          use-only-tar-bz2: true

      - run: conda create -n paper-firehose-p311 python=3.11 -y
      - run: conda install -c conda-forge pip -y
      - run: conda run -n paper-firehose-p311 pip install -r requirements.txt

      # Run the new CLI system instead of old rssparser.py
      - name: Run paper firehose pipeline
        run: |
          set -euo pipefail
          
          echo "=== Starting Paper Firehose Pipeline ==="
          
          # Check database state before starting
          echo "Pre-pipeline database check:"
          ls -la assets/*.db 2>/dev/null || echo "No database files found"
          
          # Run the complete pipeline with timeouts and debugging
          echo "Step 1: Running filter command..."
          timeout 1800 conda run -n paper-firehose-p311 python cli/main.py filter --verbose || {
            echo "❌ Filter command timed out or failed after 30 minutes"
            echo "Checking database state..."
            ls -la assets/*.db || echo "No database files found"
            echo "Checking for any running processes..."
            ps aux | grep python || echo "No python processes found"
            exit 1
          }
          
          echo "Step 2: Running rank command..."
          timeout 600 conda run -n paper-firehose-p311 python cli/main.py rank || {
            echo "❌ Rank command timed out or failed after 10 minutes"
            exit 1
          }
          
          echo "Step 3: Running abstracts command..."
          timeout 1200 conda run -n paper-firehose-p311 python cli/main.py abstracts --mailto "github-actions@paper-firehose.com" || {
            echo "❌ Abstracts command timed out or failed after 20 minutes"
            exit 1
          }
          
          echo "Step 4: Running summarize command..."
          timeout 1800 conda run -n paper-firehose-p311 python cli/main.py summarize || {
            echo "❌ Summarize command timed out or failed after 30 minutes"
            exit 1
          }
          
          echo "✅ Pipeline completed successfully"
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      # Save updated databases to cache for next run (BOOTSTRAP MODE)
      - name: Save updated databases to cache
        run: |
          set -euo pipefail
          
          echo "=== BOOTSTRAP RUN: Preparing for cache population ==="
          
          # Only all_feed_entries.db will be cached (history cache is disabled)
          if [ -f "assets/all_feed_entries.db" ]; then
            echo "✅ Updated all_feed_entries.db will be cached for next run"
            ls -lh assets/all_feed_entries.db
          fi
          
          if [ -f "assets/matched_entries_history.db" ]; then
            echo "ℹ️  Updated matched_entries_history.db (cache disabled - will be enabled after this run)"
            ls -lh assets/matched_entries_history.db
          fi
          
          echo "=== After this run, re-enable history database caching ==="

      # DATABASE SNAPSHOTTING STEP
      - name: Snapshot DBs to data branch with retention (worktree)
        env:
          KEEP_SNAPSHOTS: "10"
          SNAP_DIR: "assets/db_history"
          DB_HISTORY: "assets/matched_entries_history.db"
          DB_SEEN: "assets/all_feed_entries.db"
          DB_CURRENT: "assets/papers.db"
        run: |
          set -euo pipefail

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git fetch origin --prune

          # Ensure remote data branch exists; create from current HEAD if missing
          if ! git ls-remote --exit-code --heads origin data >/dev/null 2>&1; then
            git branch -f data HEAD
            git push -u origin data || true
          fi

          # Worktree on data branch (create/reset local branch to origin/data)
          WT=/tmp/data-wt
          mkdir -p "$WT"
          git worktree add -B data "$WT" origin/data
          cleanup() { git worktree remove -f "$WT" || true; }
          trap cleanup EXIT

          pushd "$WT" >/dev/null
            mkdir -p "$SNAP_DIR" assets
            TS="$(date -u +%Y%m%dT%H%M%SZ)"

            snapshot_one() {
              local src="$1"; local stem="$2"
              if [ -f "$GITHUB_WORKSPACE/$src" ]; then
                local snap="$SNAP_DIR/${stem}_${TS}.db"
                cp "$GITHUB_WORKSPACE/$src" "$snap"
                cp "$GITHUB_WORKSPACE/$src" "assets/${stem}.latest.db"
                git add -f "$snap" "assets/${stem}.latest.db"
              else
                echo "Skip: $src not found"
              fi
            }

            # All three DBs
            snapshot_one "$DB_HISTORY" "matched_entries_history"
            snapshot_one "$DB_SEEN" "all_feed_entries"
            snapshot_one "$DB_CURRENT" "papers"

            # Retention per DB
            retain() {
              local stem="$1"
              mapfile -t FILES < <(ls -1t "$SNAP_DIR"/${stem}_*.db 2>/dev/null || true)
              if [ "${#FILES[@]}" -gt "${KEEP_SNAPSHOTS}" ]; then
                for f in "${FILES[@]:${KEEP_SNAPSHOTS}}"; do rm -f "$f"; done
              fi
            }
            retain "matched_entries_history"
            retain "all_feed_entries"
            retain "papers"

            # Stage deletions from retention
            git add -A "$SNAP_DIR" || true

            if ! git diff --cached --quiet; then
              git commit -m "Snapshot DBs @ ${TS} [skip ci]"
              git push origin data
            else
              echo "No changes to commit."
            fi
          popd >/dev/null

      # Prepare files for GitHub Pages deployment
      - name: Prepare site
        run: |
          mkdir site
          cp -r archive site/ || true
          cp *.html site/ || true

      - uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
